{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit ('base': conda)",
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3c8c3a880c14b61cd710572e2ac6298335c33509b1f7d3900bc107f98e9cc647"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "When it comes to machine lerning problems there are a lot of diffrent types of metrics . we will see some of the most common metrics that we can use .\n",
    "\n",
    "If we talk about classification problems , the most common metrics used are : \n",
    "- Accuracy\n",
    "- Precision (P)\n",
    "- Recall (R)\n",
    "- F1 Score(F1)\n",
    "- Area Under teh ROC (Receiver Operating Characteristics)  Curve or simply AUC \n",
    "- Log loss\n",
    "- Precision at K (P@K)\n",
    "- Average precision at k (AP@k)\n",
    "- Mean average precision as K (MAP@k)\n",
    "\n",
    "when it comes to regression , the most commonly used evalation metrics are :\n",
    "\n",
    "- Mean absolute error (MAE)\n",
    "- Mean squared error (MSE)\n",
    "- Root mean squared error (RMSE)\n",
    "- Root mean Square Logrithmic error (RMSLE)\n",
    "- Mean percentage erro (MPE)\n",
    "- Mean absolute peercentage error (MAPE)\n",
    "- $ R^2 $"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python code for calculating Accuracy\n",
    "\n",
    "def accuracy(y_true ,  y_pred):\n",
    "    '''\n",
    "    Function to calculate accuracy\n",
    "    : param y_true: list of true values\n",
    "    : param y_pred: list of predicted values\n",
    "    : return: accuracy score\n",
    "    '''\n",
    "    # initialize a simple counter for correct predictions\n",
    "    correct_counter = 0\n",
    "\n",
    "    # loop over all elements of y_True and y_pred together\n",
    "    for yt , yp in zip(y_true , y_pred):\n",
    "        if yt == yp:\n",
    "            # if the prediction is equal to truth,  increase the counter\n",
    "            correct_counter += 1\n",
    "\n",
    "    # return accuracy which is correct prediction over the number of samples\n",
    "    return correct_counter / len(y_true)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.625\n0.625\n"
    }
   ],
   "source": [
    "# checking our accuracy function\n",
    "l1 = [0,1,1,1,0,1,0,1]\n",
    "l2 = [1,0,1,1,0,1,1,1]\n",
    "score = accuracy(l1 , l2)\n",
    "print(score)\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(l1,l2))"
   ]
  },
  {
   "source": [
    "Before **Precision** let's understand the Four terms i.e **True Positive , False Positive ,False Negative, Treu Neagtive**\n",
    "\n",
    "To understand these four terms let's see a senario,  assume there is a person accuse of murder, Now there are two possibility whether the accuse is guilty or innocent, Now the case will go to the court where the jugde of the court will give their verdict whether he is guilty or not.  Now here in this case there are four possibilities :\n",
    "\n",
    "**True Positive:** - The accused person is guilty and the Verdict will also be guilty\n",
    "\n",
    "**False Positive:** - The accused personis innocent and the Verdict will be guilty. This is also calle **type I** error\n",
    "\n",
    "**False Negative:** - The accused person is guilty and the verdict will be innocent. This is also called **type II** error\n",
    "\n",
    "**True Neagtive:** -  The accused person is innocent and the Verdict will also be innocent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now see the implemention of these , one at a time\n",
    "\n",
    "def true_positive(y_true , y_pred):\n",
    "    '''\n",
    "    Function to calculate True Positives\n",
    "    :Param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    : return : number of true positives\n",
    "    '''\n",
    "    # initialize\n",
    "    tp =  0\n",
    "    for yt , yp in zip(y_true , y_pred):\n",
    "        if yt == 1 and yp == 1:\n",
    "            tp += 1\n",
    "\n",
    "    return tp\n",
    "\n",
    "def true_negative(y_true , y_pred):\n",
    "    '''\n",
    "    Function to calcualte the True Negatives\n",
    "    :param y_true: list of true values\n",
    "    :param y_true: list of predicted values\n",
    "    : return : number true negatives\n",
    "    '''\n",
    "    # initialize\n",
    "    tn = 0\n",
    "    for yt , yp in zip(y_true , y_pred):\n",
    "        if yt == 0 and yp ==0:\n",
    "            tn += 1\n",
    "\n",
    "    return tn\n",
    "\n",
    "def false_positive(y_true , y_pred):\n",
    "    '''\n",
    "    Function to calculate False Positives\n",
    "    :param y_true : list of true values\n",
    "    :param y_pred : list of predicted values\n",
    "    :return :  number of false positives\n",
    "    '''\n",
    "    # initialize\n",
    "\n",
    "    fp = 0\n",
    "    for yt , yp in zip(y_true , y_pred):\n",
    "        if yt == 0 and yp ==1:\n",
    "            fp += 1\n",
    "\n",
    "    return fp\n",
    "\n",
    "def false_negative(y_true , y_pred):\n",
    "    '''\n",
    "    Function to calculate False Negatives\n",
    "    :param y_true :  list of true values\n",
    "    :param y_pred : list of predicted values\n",
    "    :return : number of false negatives\n",
    "    '''\n",
    "     # intialize \n",
    "\n",
    "    fn = 0\n",
    "    for yt , yp in zip(y_true , y_pred):\n",
    "         if yt == 1 and yp == 0:\n",
    "             fn += 1\n",
    "    return fn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "True positive: 4\nFalse positive: 1\nFalse Positive: 2\nTrue negative: 1\n"
    }
   ],
   "source": [
    "# checking our accuracy function\n",
    "l1 = [0,1,1,1,0,1,0,1]\n",
    "l2 = [1,0,1,1,0,1,1,1]\n",
    "print('True positive:',true_positive(l1 , l2))\n",
    "print('False positive:',false_negative(l1,l2))\n",
    "print('False Positive:',false_positive(l1 , l2))\n",
    "print('True negative:',true_negative(l1,l2))"
   ]
  },
  {
   "source": [
    "$ Precision  =  \\frac{tp}{tp + fp}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true , y_pred):\n",
    "    '''\n",
    "    Function to calculate precision\n",
    "    :param y_true : list of true values\n",
    "    :param y_pres : list of predicted values\n",
    "    : return : precision score\n",
    "    '''\n",
    "    tp = true_positive(y_true ,  y_pred)\n",
    "    fp = false_positive(y_true , y_pred)\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.6666666666666666"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# checking our precision function\n",
    "l1 = [0,1,1,1,0,1,0,1]\n",
    "l2 = [1,0,1,1,0,1,1,1]\n",
    "precision(l1 , l2)"
   ]
  },
  {
   "source": [
    "$ Recall = \\frac{tp}{tp + fn}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true ,  y_pred):\n",
    "    '''\n",
    "    Function for calculating recall\n",
    "    :param y_true :  list  of true values\n",
    "    :param y_pred :  list of flase values\n",
    "    : return : recall score\n",
    "    '''\n",
    "    tp = true_positive(y_true ,  y_pred)\n",
    "    fn = false_negative(y_true,  y_pred)\n",
    "    recall = tp / (tp + fn)\n",
    "    return recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# checking our recall function\n",
    "l1 = [0,1,1,1,0,1,0,1]\n",
    "l2 = [1,0,1,1,0,1,1,1]\n",
    "recall(l1 , l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}