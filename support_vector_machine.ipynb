{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "\n",
    "# Support Vector Machine\n",
    "\n",
    "“Support Vector Machine” (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well (Look at the picture below)\n",
    "\n",
    "![SVM](./picture/SVM_1.png)\n",
    "\n",
    "Support Vectors are simply the co-ordinates of individual observation. The SVM classifier is a frontier which best segregates the two classes (hyper-plane/ line).\n",
    "\n",
    "Above, we got accustomed to the process of segregating the two classes with a hyper-plane. Now the burning question is “How can we identify the right hyper-plane?”.\n",
    "\n",
    "- **Identify the right hyper-plane (Scenario-1):** Here, we have three hyper-planes (A, B and C). Now, identify the right hyper-plane to classify star and circle.\n",
    "\n",
    "![SVM](./picture/SVM_2.png)\n",
    "\n",
    "You need to remember a thumb rule to identify the right hyper-plane: “Select the hyper-plane which segregates the two classes better”. In this scenario, hyper-plane “B” has excellently performed this job.\n",
    "\n",
    "- **Identify the right hyper-plane (Scenario-2):** Here, we have three hyper-planes (A, B and C) and all are segregating the classes well. Now, How can we identify the right hyper-plane?\n",
    "\n",
    "![SVM](./picture/SVM_3.png)\n",
    "\n",
    "Here, maximizing the distances between nearest data point (either class) and hyper-plane will help us to decide the right hyper-plane. This distance is called as Margin. Let’s look at the below snapshot:\n",
    "\n",
    "![SVM](./picture/SVM_4.png)\n",
    "\n",
    "Above, you can see that the margin for hyper-plane C is high as compared to both A and B. Hence, we name the right hyper-plane as C. Another lightning reason for selecting the hyper-plane with higher margin is robustness. If we select a hyper-plane having low margin then there is high chance of miss-classification.\n",
    "\n",
    "- **Identify the right hyper-plane (Scenario-3):** Hint: Use the rules as discussed in previous section to identify the right hyper-plane\n",
    "\n",
    "![SVM](./picture/SVM_5.png)\n",
    "\n",
    "Some of us may have selected the hyper-plane B as it has higher margin compared to A. But, here is the catch, SVM selects the hyper-plane which classifies the classes accurately prior to maximizing margin. Here, hyper-plane B has a classification error and A has classified all correctly. Therefore, the right hyper-plane is A.\n",
    "\n",
    "\n",
    "## Mathematical Part of SVM\n",
    "\n",
    "The predicted label for some input feature vector **\"X\"** is given like this:-\n",
    "$$ Y = sign(W*X - B)$$\n",
    "\n",
    "where , **\"sing\"** is a mathematical operator that takes any values as input and returns **+1** if the input is a positive number or **-1** if the input is a negative number\n",
    "\n",
    "The goal of the learning algorithm is to leverage the dataset and find the optimal values **\"W\"** and **\"B\"** for parameter W and B. \n",
    "Once the learning algorithm identifies these optimal values, the model $f(X)$ is defined as:\n",
    "$$ F(X) = sign(W*X + B)$$\n",
    "\n",
    "Now, how does the machine find **\"W\"** and **\"B\"** ?\n",
    "\n",
    "It solves an optimization problem, Machine are good at optimiing function under constraints\n",
    "So, the constraints are naturally:-\n",
    "\n",
    "$$ W*X_i - B >= 1 $$ if $Y_i = +-1$\n",
    "and $$ W*X_i -b <= -1 $$ if $ Y_i = -1\n",
    "\n",
    "We would also prefer that the hyperplane seperates positive example from negative example with the largest possible margine\n",
    "- The margine is the distance between the closest examples of two classes.\n",
    "A large margine contributes to a better generalization, that is how well model will classify new examples in the future\n",
    "To Achieve that, we need to maximize the Euclidean norm of **||W||** and given by :-\\\n",
    "$$ \\sqrt{\\sum_{j=0}^D(W^{(j)})^2}$$\n",
    "\n",
    "Geometrically, the equations $W*X - B = 1$ and $ W*X - B = -1$ defined two parallel hyperplanes.\n",
    "The distance between these hyperplanes is given by $\\frac{2}{||W||}$ , So the smaller the norm **||W||** , the larger the distance between these two hyperplanes.\n",
    "\n",
    "Minimizing ||W|| is equivalent to manimizing $\\frac{1}{2}||W||^2$ , and the use of this term makes it possible to perform quadratic programming optimization later on.\n",
    "\n",
    "The optimization problem for **SVM** therefore looks like:\n",
    "$$ min\\frac{1}{2}||W||^2 $$ \n",
    "such that , $ Y_i(X_iW - B) - 1 >= 0$, where i = 1,2,3,....,N\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}