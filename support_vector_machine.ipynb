{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "\n",
    "# Support Vector Machine\n",
    "\n",
    "“Support Vector Machine” (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well (Look at the picture below)\n",
    "\n",
    "![SVM](./picture/SVM_1.png)\n",
    "\n",
    "Support Vectors are simply the co-ordinates of individual observation. The SVM classifier is a frontier which best segregates the two classes (hyper-plane/ line).\n",
    "\n",
    "Above, we got accustomed to the process of segregating the two classes with a hyper-plane. Now the burning question is “How can we identify the right hyper-plane?”.\n",
    "\n",
    "- **Identify the right hyper-plane (Scenario-1):** Here, we have three hyper-planes (A, B and C). Now, identify the right hyper-plane to classify star and circle.\n",
    "\n",
    "![SVM](./picture/SVM_2.png)\n",
    "\n",
    "You need to remember a thumb rule to identify the right hyper-plane: “Select the hyper-plane which segregates the two classes better”. In this scenario, hyper-plane “B” has excellently performed this job.\n",
    "\n",
    "- **Identify the right hyper-plane (Scenario-2):** Here, we have three hyper-planes (A, B and C) and all are segregating the classes well. Now, How can we identify the right hyper-plane?\n",
    "\n",
    "![SVM](./picture/SVM_3.png)\n",
    "\n",
    "Here, maximizing the distances between nearest data point (either class) and hyper-plane will help us to decide the right hyper-plane. This distance is called as Margin. Let’s look at the below snapshot:\n",
    "\n",
    "![SVM](./picture/SVM_4.png)\n",
    "\n",
    "Above, you can see that the margin for hyper-plane C is high as compared to both A and B. Hence, we name the right hyper-plane as C. Another lightning reason for selecting the hyper-plane with higher margin is robustness. If we select a hyper-plane having low margin then there is high chance of miss-classification.\n",
    "\n",
    "- **Identify the right hyper-plane (Scenario-3):** Hint: Use the rules as discussed in previous section to identify the right hyper-plane\n",
    "\n",
    "![SVM](./picture/SVM_5.png)\n",
    "\n",
    "Some of us may have selected the hyper-plane B as it has higher margin compared to A. But, here is the catch, SVM selects the hyper-plane which classifies the classes accurately prior to maximizing margin. Here, hyper-plane B has a classification error and A has classified all correctly. Therefore, the right hyper-plane is A.\n",
    "\n",
    "\n",
    "## Mathematical Part of SVM\n",
    "\n",
    "The predicted label for some input feature vector **\"X\"** is given like this:-\n",
    "$$ Y = sign(W*X - B)$$\n",
    "\n",
    "where , **\"sing\"** is a mathematical operator that takes any values as input and returns **+1** if the input is a positive number or **-1** if the input is a negative number\n",
    "\n",
    "The goal of the learning algorithm is to leverage the dataset and find the optimal values **\"W\"** and **\"B\"** for parameter W and B. \n",
    "Once the learning algorithm identifies these optimal values, the model $f(X)$ is defined as:\n",
    "$$ F(X) = sign(W*X + B)$$\n",
    "\n",
    "Now, how does the machine find **\"W\"** and **\"B\"** ?\n",
    "\n",
    "It solves an optimization problem, Machine are good at optimiing function under constraints\n",
    "So, the constraints are naturally:-\n",
    "\n",
    "$$ W*X_i - B >= 1 $$ if $Y_i = +1$\n",
    "and $$ W*X_i -B <= -1 $$ if $ Y_i = -1$\n",
    "\n",
    "We would also prefer that the hyperplane seperates positive example from negative example with the largest possible margine\n",
    "- The margine is the distance between the closest examples of two classes.\n",
    "A large margine contributes to a better generalization, that is how well model will classify new examples in the future\n",
    "To Achieve that, we need to maximize the Euclidean norm of **||W||** and given by :-\\\n",
    "$$ \\sqrt{\\sum_{j=0}^D(W^{(j)})^2}$$\n",
    "\n",
    "Geometrically, the equations $W*X - B = 1$ and $ W*X - B = -1$ defined two parallel hyperplanes.\n",
    "The distance between these hyperplanes is given by $\\frac{2}{||W||}$ , So the smaller the norm **||W||** , the larger the distance between these two hyperplanes.\n",
    "\n",
    "Minimizing ||W|| is equivalent to manimizing $\\frac{1}{2}||W||^2$ , and the use of this term makes it possible to perform quadratic programming optimization later on.\n",
    "\n",
    "The optimization problem for **SVM** therefore looks like:\n",
    "$$ min\\frac{1}{2}||W||^2 $$ \n",
    "such that , $ Y_i(X_iW - B) - 1 >= 0$, where i = 1,2,3,....,N\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's see how to perform the Support Vector Classification Using **SKLearn** library\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "df = pd.read_csv('dataset/breast_cancer_wisconsin.data')\n",
    "df.replace('?' , -99999 ,inplace =True)\n",
    "df.drop(['id'] , 1 , inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   clump_thickness  unif_cell_size  unif_cell_shape  marg_adhesion  \\\n0                5               1                1              1   \n1                5               4                4              5   \n2                3               1                1              1   \n3                6               8                8              1   \n4                4               1                1              3   \n\n   single_epith_cell_size bare_nuclei  bland_chrom  norm_nucleoli  mitoses  \\\n0                       2           1            3              1        1   \n1                       7          10            3              2        1   \n2                       2           2            3              1        1   \n3                       3           4            3              7        1   \n4                       2           1            3              1        1   \n\n   class  \n0      2  \n1      2  \n2      2  \n3      2  \n4      2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>clump_thickness</th>\n      <th>unif_cell_size</th>\n      <th>unif_cell_shape</th>\n      <th>marg_adhesion</th>\n      <th>single_epith_cell_size</th>\n      <th>bare_nuclei</th>\n      <th>bland_chrom</th>\n      <th>norm_nucleoli</th>\n      <th>mitoses</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>4</td>\n      <td>4</td>\n      <td>5</td>\n      <td>7</td>\n      <td>10</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>8</td>\n      <td>8</td>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>3</td>\n      <td>7</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(df.drop(['class'], 1 ))\n",
    "Y = np.array(df['class'])\n",
    "\n",
    "x_train , x_test, y_train, y_test = train_test_split(X , Y , test_size = 0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.6642857142857143\n"
    }
   ],
   "source": [
    "clf = SVC()\n",
    "clf.fit(x_train , y_train)\n",
    "accuracy = clf.score(x_test , y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "source": [
    "## SVM from Scratch:-\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import style\n",
    "import numpy as np \n",
    "style.use('ggplot')\n",
    "\n",
    "data_dict = {-1 : np.array([[1 , 7],\n",
    "                            [2 , 8],\n",
    "                            [3 , 8],]), \n",
    "              1 : np.array([[5 , 1],\n",
    "                            [6 , -1],\n",
    "                            [7 , 3],])}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Support_Vector_Machine:\n",
    "    def __init__(self , visulazation = True):\n",
    "        self.visulazation = visulazation\n",
    "        self.colors = {1 : 'r' , -1 : 'b'}\n",
    "        if self.visulazation:\n",
    "            self.fig = plt.figure()\n",
    "            self.ax = self.fig.add_subplot(1,1,1)\n",
    "    # Train\n",
    "    def fit(self , data):\n",
    "        self.data = data\n",
    "        opt_dict = {}\n",
    "        # {||w|| :  [w,b]}\n",
    "        transforms = [[1,1],\n",
    "                     [-1,1],\n",
    "                     [1,-1],\n",
    "                     [-1,-1]]\n",
    "        all_data = []\n",
    "        for yi in self.data:\n",
    "            for featureset in self.data[yi]:\n",
    "                for feature in featureset:\n",
    "                    all_data.append(feature)\n",
    "\n",
    "\n",
    "        self.max_feature_value = max(all_data)\n",
    "        self.min_feature_value = min(all_data)\n",
    "        all_data = None\n",
    "\n",
    "\n",
    "        step_sizes = [self.max_feature_value*0.1,\n",
    "                      self.max_feature_value*0.01,\n",
    "                      # point of expense\n",
    "                      self.max_feature_value*0.001,]\n",
    "        # extremly expensive\n",
    "        b_range_multiple = 5\n",
    "        # we don't need to take as small of steps\n",
    "        # with b as we do w\n",
    "        b_multiple = 5\n",
    "        latest_optimum = self.max_feature_value*10\n",
    "\n",
    "        for step in step_sizes:\n",
    "            w = np.array([latest_optimum , latest_optimum])\n",
    "            # we can do this because convex\n",
    "            optimized = False\n",
    "\n",
    "            while not optimized:\n",
    "\n",
    "                for b in np.arange(-1*(self.max_feature_value * b_range_multiple),\n",
    "                                    self.max_feature_value * b_range_multiple , step * b_multiple):\n",
    "                    for transformation in transforms:\n",
    "                        w_t = w * transformation\n",
    "                        found_option = True\n",
    "                        # weakest link in the SVM fundamentally\n",
    "                        # SMO attempts to fix this a bit\n",
    "                        # yi(xi.w + b) >= 1\n",
    "                        #\n",
    "                        # ##### add a break here later...\n",
    "                        for i in self.data:\n",
    "                            for xi in self.data[i]:\n",
    "                                yi = i\n",
    "                                if not yi*(np.dot(w_t , xi) + b) >=1:\n",
    "                                    found_option = False\n",
    "                        if found_option:\n",
    "                            opt_dict[np.linalg.norm(w_t)] = [w_t , b]\n",
    "\n",
    "                if w[0] < 0:\n",
    "                    optimized = True\n",
    "                    print(' optimized a step.')\n",
    "                else:\n",
    "                    w = w - step \n",
    "\n",
    "        norms = sorted([n for n in opt_dict])\n",
    "        # ||w|| : [w,b]\n",
    "        opt_choice = opt_dict[norms[0]]\n",
    "        self.w = opt_choice[0]\n",
    "        self.b = opt_choice[1]\n",
    "        latest_optimum = opt_choice[0][0] + step * 2\n",
    "\n",
    "\n",
    "    def predict(self , features):\n",
    "        # sign(x.w + b)\n",
    "        classification = np.sign(np.dot(np.array(features) , self.w ) + self.b)\n",
    "\n",
    "        return classification\n"
   ]
  }
 ]
}