{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine Building a model for binary judgment, Is this mail spam or not? ,  Should we hire this candiate or not?\n",
    "\n",
    "All the result lies in one of the four categories namely:- \n",
    "\n",
    "- **True Positive :-**  \" This message is spam and we predicted it as spam\"\n",
    "- **False Positive (Type 1 Error):-** \" This message is not spam and we predicted it as spam\"\n",
    "- ** False Negative (Type 2 Error):-** \" This message is spam and we predicted it as not spam\"\n",
    "- **True Negative :- ** \"This message is not spam and we predicted it as not spam\"\n",
    "\n",
    "**We often represent these as counts in confusion matrix :-**\n",
    "\n",
    "|                  | **Spam**       | **Not Spam** |\n",
    "|:----------------:|:--------------:|:------------:|\n",
    "|Predict \"spam\"    | True Positive  |False Positive|\n",
    "|Predict \"Not Spam\"| False Negative |True Negative |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.98114\n"
    }
   ],
   "source": [
    "# We use these in calculating various Statistics about model per formances\n",
    "\n",
    "tp , fp , fn , tn = 70 , 4930, 13930, 981070\n",
    "\n",
    "def accuracy (tp , fp , fn , tn):\n",
    "    correct = tp + tn\n",
    "    total = tp + fp + fn + tn\n",
    "    return correct/total\n",
    "\n",
    "print(accuracy(tp , fp , fn , tn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "98.114% Seems like a pretty impressive number. But clearly this is not a good test, which means that we probably shouldn't put a lot of credence in raw accuracy.\n",
    "\n",
    "**It's common to look at the combination of *Pricision* and *recall* **\n",
    "\n",
    "- Precision measures how accurate our positive predictions were:\n",
    "\n",
    "$Precision = \\frac{tp}{tp + fp}$\n",
    "\n",
    "- And recall measures what fraction of the positives our model idetified:-\n",
    "\n",
    "$Recall = \\frac{tp}{tp + fn}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.014\n0.005\n"
    }
   ],
   "source": [
    "def precision (tp , fp , fn , tn):\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "print(precision(tp, fp, fn, tn))\n",
    "\n",
    "def recall (tp , fp , fn , tn):\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "print(recall(tp , fp , fn, tn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are both terrible numbers, reflecting  that this is a terrible model.\n",
    "Sometimes *precision* and *recall* are combined into the *F1* Score, which is defined as:\n",
    "\n",
    "${F1\\_score} = \\frac{2.P.R}{P + R}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.00736842105263158\n"
    }
   ],
   "source": [
    "def f1_score(tp , fp , fn , tn):\n",
    "    p = precision(tp , fp , fn , tn)\n",
    "    r = recall(tp , fp , fn , tn)\n",
    "    return 2*p*r/(p + r)\n",
    "\n",
    "print(f1_score(tp , fp , fn , tn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the **Harmonic mean** of precision and recall and necessarily lies between them (precision and recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}